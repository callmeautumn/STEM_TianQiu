{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEM Week 8 Processing Signals\n",
    "\n",
    "This week we will learn about several statistical techniques used on audio, image or sensor data. \n",
    "\n",
    "Often these are used as pre-processing techniques to regularise often noisy data for further processing (e.g. machine learning).\n",
    "\n",
    "They can also be used to highlight differences in a signal. This means that when we try to get a learning algorithm to model them, its job is easier.\n",
    "\n",
    "When used in this way to help construct a dataset, we might call this approaches **feature engineering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio\n",
    "\n",
    "Adapted from [this tutorial](https://colab.research.google.com/github/stevetjoa/musicinformationretrieval.com/blob/gh-pages/basic_feature_extraction.ipynb#scrollTo=dp0cfkzSNCXo)\n",
    "\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "Somehow, we must extract the characteristics of our audio signal that are most relevant to the problem we are trying to solve. For example, if we want to classify instruments by timbre, we will want features that distinguish sounds by their timbre and not their pitch. If we want to perform pitch detection, we want features that distinguish pitch and not timbre.\n",
    "\n",
    "This process is known as feature extraction.\n",
    "\n",
    "First we'll load in two sets of audio files, each representing a different drum type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa, sklearn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to search through a folder, and return the paths of any files that meet a certain pattern. This is a really common task for loading in datasets from file.\n",
    "\n",
    "We are going to see first how we can find that list, then how we can use that list to make a new collection of loaded in audio files.\n",
    "\n",
    "#### `glob`\n",
    "\n",
    "This will search a given directory for files matching a pattern and return a list of file paths.\n",
    "\n",
    "We can use the `*` wildcard symbol to mean `anything in between`\n",
    "\n",
    "#### `List Comprehensions`\n",
    "\n",
    "We're going to use something called a List comprehension in this next step. These are a quick way of making new arrays. They follow the format \n",
    "\n",
    "`[function for item in list]`\n",
    "\n",
    "This is the same as \n",
    "\n",
    "```\n",
    "for item in list:\n",
    "    function(item)\n",
    "```\n",
    "\n",
    "We can also save the result \n",
    "\n",
    "```\n",
    "new_list = [function(item) for item in old_list] \n",
    "```\n",
    "\n",
    "Where `new_list` is the result of taking all of the elements in `old_list` and passing it as an argument to `function`. The code below does the same thing using a for loop.\n",
    "\n",
    "```\n",
    "new_list = []\n",
    "for item in list:\n",
    "    new_item = function(item)\n",
    "    new_list.add(new_item)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get file paths and load audio, saving the result in a new array\n",
    "hihat_filepaths = Path().glob('audio/drum_hits/hihat_*.WAV')\n",
    "hihat_signals = [librosa.load(p)[0] for p in hihat_filepaths]\n",
    "\n",
    "snare_filepaths = Path().glob('audio/drum_hits/snare_*.WAV')\n",
    "snare_signals = [librosa.load(p)[0] for p in snare_filepaths]\n",
    "\n",
    "print(len(hihat_signals),len(snare_signals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the waveforms of the first 10\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, x in enumerate(snare_signals[:10]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    librosa.display.waveshow(x[:10000], color = \"red\")\n",
    "    plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the waveforms of the first 10\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, x in enumerate(hihat_signals[:10]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    librosa.display.waveshow(x[:10000], color = \"blue\")\n",
    "    plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features\n",
    "\n",
    "A feature vector is simply a **list of numbers** which represent **each item** in our dataset. It should provide highlight properties that are useful for whatever analysis we are doing. \n",
    "\n",
    "#### Zero Crossing Rate \n",
    "\n",
    "Tells us how often the signal is going from **negative to positive**, and vice versa.\n",
    "\n",
    "Zero crossing rate can be used as a feature to help distinguish between different instruments or sound sources. Instruments with different timbres often exhibit distinct zero crossing rate patterns. For example, percussive instruments like **drums** may have a **higher zero crossing rate** compared to **sustained instruments like strings or woodwinds**.\n",
    "\n",
    "#### Spectral Centroid\n",
    "\n",
    "Spectral centroid is commonly used to describe the **timbral characteristics of audio signals**. It can help distinguish between sounds that are **bright** or **sharp** in timbre (higher centroid values) and sounds that are **dull** or **mellow** (lower centroid values). For example, a violin's sound typically has a higher spectral centroid than that of a bass drum.\n",
    "\n",
    "This formula shows us how to calculate the Spectral Centroid, given we have first calculated an `fft`. \n",
    "\n",
    "For each bin, we take the centre frequency of that bin (`f(n)`), and multiply it by the magnitude (`X(n)`) (e.g. the strength of the signal in that bin). We sum this up and divide by the sum of all the magnitudes.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{Spectral Centroid} = \\frac{\\sum_{n=0}^{N}{f(n) \\cdot X(n)}}{\\sum_{n=0}^{N}{X(n)}}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.feature.zero_crossing_rate(y=hihat_signals[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.feature.spectral_centroid(y=hihat_signals[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple function that constructs a two-dimensional feature vector from a signal. \n",
    "\n",
    "Each feature extractor returns **an array of values over a number of frames**. We take the **mean** of that array to return 3 numbers for each audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(signal):\n",
    "    return [\n",
    "        librosa.feature.zero_crossing_rate(y=signal).mean(),\n",
    "        librosa.feature.spectral_centroid(y=signal, n_fft=512).mean(),\n",
    "        librosa.feature.rms(y=signal).mean()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to aggregate all of the feature vectors among signals in a collection, we can use a list comprehension as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hihat_features = np.array([extract_features(x) for x in hihat_signals])\n",
    "snare_features = np.array([extract_features(x) for x in snare_signals])\n",
    "print(hihat_features.shape, snare_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the differences in features by plotting separate histograms for each of the classes: First we see how amplitude information (``RMS`` or ``root-mean-squared``) doesnt really distinguish between the two classes (lots of cross over between red and blue histograms). Our next two (``zero crossing rate`` and `spectral centroid`) are much better feature representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.hist(hihat_features[:,2], color='b', range=(0, 0.3), alpha=0.5, bins=20)\n",
    "plt.hist(snare_features[:,2], color='r', range=(0, 0.3), alpha=0.5, bins=20)\n",
    "plt.legend(('hihats', 'snares'))\n",
    "plt.xlabel('RMS')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.hist(hihat_features[:,0], color='b', range=(0, 0.8), alpha=0.5, bins=20)\n",
    "plt.hist(snare_features[:,0], color='r', range=(0, 0.8), alpha=0.5, bins=20)\n",
    "plt.legend(('hihats', 'snares'))\n",
    "plt.xlabel('Zero Crossing Rate')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.hist(hihat_features[:,1], color='b', range=(0, 10000), alpha=0.5, bins=20)\n",
    "plt.hist(snare_features[:,1], color='r', range=(0,10000), alpha=0.5, bins=20)\n",
    "plt.legend(('hihats', 'snares'))\n",
    "plt.xlabel('Spectral Centroid')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "The features that we used in the previous example included zero crossing rate and spectral centroid. These two features are expressed using different units. This discrepancy can pose problems when performing classification later. Therefore, we will normalize each feature vector to a common range and store the normalization parameters for later use.  \n",
    "\n",
    "Many techniques exist for scaling your features. For now, we'll use [`sklearn.preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).  `MinMaxScaler` returns an array of scaled values such that each feature dimension is in the range 0 to 1.\n",
    "\n",
    "The formula below describes how we would calculate each new scaled point. We would subtract the minimum value in the sequence, then divide each point by the difference between the maximum and minimum values\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\text{Min-Max Scaling:} \\quad X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's concatenate all of our feature vectors into one *feature table*. We need them to both be in one collection in order to do the above calculation properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table = np.vstack((hihat_features, snare_features))\n",
    "print(feature_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale each feature dimension to be in the range 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "training_features = scaler.fit_transform(feature_table)\n",
    "print(training_features.min(axis=0))\n",
    "print(training_features.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = len(hihat_features)\n",
    "plt.scatter(training_features[:split,0], training_features[:split,1], c='b')\n",
    "plt.scatter(training_features[split:,0], training_features[split:,1], c='r')\n",
    "plt.xlabel('Zero Crossing Rate')\n",
    "plt.ylabel('Spectral Centroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "Adapted from [Mick Griersons notebook](https://github.com/ual-cci/MSc-Coding-2/blob/master/Week-6-Exercise-intro-to-image-data-and-tensorflow.ipynb)\n",
    "\n",
    "We are going to look at ways of preprocessing an image dataset. This can itself reveal interesting things about the data, as well as making later machine learning easier by highlighting where the differences between our images are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = Path().glob('images/originals/*.jpg')\n",
    "full_portraits = np.array([plt.imread(p) for p in image_paths])\n",
    "len(full_portraits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 18 random pictures \n",
    "indexes = np.random.randint(0, len(full_portraits), 18)\n",
    "images_to_show = full_portraits[indexes]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for index, image in enumerate(images_to_show):\n",
    "    #display on a grid of 3 rows and 6 columns (3x6=18)\n",
    "    plt.subplot(3, 6, index+1)  \n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much variation is in the dataset?\n",
    "\n",
    "We can look at the `mean` of the dataset along the first axis in order to see the average picture. The result is quite a blurry one with not much distinguishing features, suggesting that there is little consensus to our dataset when think about each picture laid on top of each other.\n",
    "\n",
    "We can also view the `standard deviation` of our dataset. Again, this is fairly uniform and doesnt really highlight any areas of the canvas that have more variation than others.\n",
    "\n",
    "If we are trying to highlight where the interesting parts will be in our dataset, this isn't a particularly useful representation as theres so much variation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "Standard Deviation tells us how much variance there is a set of data\n",
    "\n",
    "**Variance** is the mean distance from the mean of the group\n",
    "\n",
    "**Standard Deviation** is the square root of this\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trig = cv2.imread('images/stdev.jpg')\n",
    "plt.imshow(trig.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = np.mean(full_portraits, axis=0) # This is the mean of the 'batch' channel\n",
    "plt.imshow(mean_img.astype(np.uint8))\n",
    "print(\"look at this average person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_img = np.std(full_portraits, axis=0)\n",
    "plt.imshow(std_img.astype(np.uint8))\n",
    "print(\"This is the standard deviation - the variance of the mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Faces\n",
    "\n",
    "One thing that will help us here is to find the faces and align them with the eyes parallel to the base. When we know that all the images are of mainly faces, and with similar orientations, then hopefully the `mean` and `standard deviation` will reveal some interesting information about where the variation in the faces is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in aligned dataset\n",
    "faces = np.array([plt.imread(p) for p in Path().glob('images/aligned_faces/*.jpg')])\n",
    "len(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 18 random pictures \n",
    "indexes = np.random.randint(0, len(faces), 18)\n",
    "images_to_show = faces[indexes]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for index, image in enumerate(images_to_show):\n",
    "    #display on a grid of 3 rows and 6 columns (3x6=18)\n",
    "    plt.subplot(3, 6, index+1)  \n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = np.mean(faces, axis=0) # This is the mean of the 'batch' channel\n",
    "plt.imshow(mean_img.astype(np.uint8))\n",
    "print(\"look at this average person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately we see a much clearer face like structure in our mean picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_img = np.std(faces, axis=0)\n",
    "plt.imshow(std_img.astype(np.uint8))\n",
    "print(\"This is the standard deviation - the variance of the mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just shown where changes are likely to be in our dataset of images. Or put another way, we're showing where and how much variance there is in our previous mean image representation.\n",
    "\n",
    "We're looking at this per color channel. So we'll see variance for each color channel represented separately, and then combined as a color image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, we can try to look at the average variance over all color channels by taking their mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.mean(std_img, axis=2).astype(np.uint8)) # Mean of all colour channels (keep the spatial dimensions across wxh, squash c)\n",
    "print(\"Mean of all colour channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is showing us on average, how each pixel will vary as a heatmap. The more red, the more variation in the dataset and the less useful the mean is at representing it. The more blue, the more likely that our mean image is representative of the interesting parts of the images in our dataset.\n",
    "\n",
    "### How did we align the faces?\n",
    "\n",
    "First we used `openCV` to get the bounding boxes for the face and the eyes in the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faces import get_face, get_eyes\n",
    "full_portrait = plt.imread(\"images/portraits/spyros-papaloukas_3.jpg\")\n",
    "face, fx, fy, fw, fh = get_face(full_portrait.copy())\n",
    "plt.imshow(face.astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes, left_eye_x,left_eye_y, right_eye_x, right_eye_y = get_eyes(full_portrait.copy(),fx,fy,fw,fh)\n",
    "plt.imshow(eyes.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigonometry Refresher \n",
    "\n",
    "Here we have a right angled triangle, it has 3 angles and 3 sides. If we know some of these things, then we are able to work out some of the **unknowns**. \n",
    "\n",
    "We name the sides of the triangle in the relation to the **right angle**. This is the angle which is 90 degrees in te corner. The three respective sides are \n",
    "\n",
    " - Adjacent \n",
    " - Opposite \n",
    " - Hypotenuse \n",
    "\n",
    "### Calculating Theta \n",
    "\n",
    "What we want to know is the angle in the corner of the triangle (shown by the greek symbol **Theta θ**). This will allow us to rotate the image back so that the face is aligned parallel in the image. \n",
    "\n",
    "If we look at the triangle we have made between the eyes, we know that have the values for the **adjacent** and **opposite** sides. Using the above rules, we can see that \n",
    "\n",
    "tan(θ) = opp/adj\n",
    "\n",
    "And by doing a bit of algebra, we do the opposite operation to tan to both sides to solve for θ. This is called **arctan**, and this exists for both sin (**arcsin**) and cos (**arccos**).\n",
    "\n",
    "θ = arctan(opp/adj)\n",
    "\n",
    "For all types of problems involving **right angled** triangles and you are looking to calculate **lengths** or **angles**, you can use the appropriate formula depending on what you know, and what you need to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trig = cv2.imread('images/sohcahtoa.jpg')\n",
    "plt.imshow(trig.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent = right_eye_x - left_eye_x\n",
    "opposite = right_eye_y - left_eye_y\n",
    "angle = np.arctan(opposite/adjacent)\n",
    "#Convert from radians into degrees\n",
    "angle = (angle * 180) / np.pi\n",
    "h, w = full_portrait.shape[:2]\n",
    "center = (int(fx),int(fy))\n",
    "M = cv2.getRotationMatrix2D(center, (angle), 1.0)\n",
    "rotated = cv2.warpAffine(full_portrait, M, (w, h))\n",
    "face = rotated[fy:(fy+fh), fx:(fx+fw)]\n",
    "plt.imshow(face.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Let's have a look at our dataset another way to see why this might be a useful thing to do.\n",
    "\n",
    " We're first going to convert our batch x height x width x channels array into a 1 dimensional array. Instead of having 4 dimensions, we'll now just have 1 dimension of every pixel value stretched out in a long vector, or 1 dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = faces.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert our N x H x W x C dimensional array into a 1 dimensional array.  The values of this array will be based on the last dimensions order.  So we'll have: [<font color='red'>251</font>, <font color='green'>238</font>, <font color='blue'>205</font>, <font color='red'>251</font>, <font color='green'>238</font>, <font color='blue'>206</font>, <font color='red'>253</font>, <font color='green'>240</font>, <font color='blue'>207</font>, ...]\n",
    "\n",
    "We can visualize what the \"distribution\", or range and frequency of possible values are. This is a very useful thing to know. It tells us whether our data is predictable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.hist(flattened, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 255\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\n",
    "axs[0].hist(flattened[::3], bins)\n",
    "axs[0].set_title('r')\n",
    "axs[1].hist(flattened[1::3], bins)\n",
    "axs[1].set_title('g')\n",
    "axs[2].hist(flattened[2::3], bins)\n",
    "axs[2].set_title('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bars of each bin describe the frequency, or how many times anything **within that range of values appears**. In other words, it is telling us if there is something that seems to happen **more than anything else**. If there is, it is likely that a machine learning algorithm can take advantage of that.\n",
    "\n",
    "### Histogram Equalization\n",
    "\n",
    "The mean of our dataset looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plt.hist(mean_img.ravel(), 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we **subtract** an our mean image from something in the dataset, we **remove all of this information from it**. And that means that the rest of the information is really what is important for describing what is **unique** about it.\n",
    "\n",
    "Let's take a single image (the second one (`faces[1]`)) and compare the histogram **before** and **after** ``normalizing our data``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 20\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\n",
    "axs[0].hist((faces[1]).ravel(), bins)\n",
    "axs[0].set_title('img distribution')\n",
    "axs[1].hist((mean_img).ravel(), bins)\n",
    "axs[1].set_title('mean distribution')\n",
    "axs[2].hist((faces[1] - mean_img).ravel(), bins)\n",
    "axs[2].set_title('(img - mean) distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see from the histograms is the original image's distribution of values from 0 - 255. The mean image's data distribution is mostly **centered around the value 100**. \n",
    "\n",
    "When we look at the difference of the original image and the mean image as a histogram, we can see that the **distribution is now centered around 0**. What we are seeing is the distribution of values that were **above the mean image's intensity,** and which were **below** it. \n",
    "\n",
    "Images can have variations in lighting conditions, contrast, and overall brightness. By centering the data, you make the model less **sensitive to these variations** because the mean brightness is effectively subtracted, allowing the model to focus on **distinguishing image content** rather than absolute pixel values.\n",
    "\n",
    "Let's take it one step further and complete the normalization by dividing by the standard deviation of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\n",
    "axs[0].hist((faces[1] - mean_img).ravel(), bins)\n",
    "axs[0].set_title('(img - mean) distribution')\n",
    "axs[1].hist((std_img).ravel(), bins)\n",
    "axs[1].set_title('std deviation distribution')\n",
    "axs[2].hist(((faces[1] - mean_img) / std_img).ravel(), bins)\n",
    "axs[2].set_title('((img - mean) / std_dev) distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that the data is in the range of -3 to 3, with the **bulk of the data centered around -1 to 1**. This is the effect of **normalizing** our data: most of the data will be around 0, where some deviations of it will follow between -3 to 3.\n",
    "\n",
    "This has many of the advantages of **matching scale** as other normalisation methods. Additionally, standard deviation quantifies the spread or dispersion of data points. \n",
    "\n",
    "When you divide by the standard deviation, you're essentially expressing each data point in terms of how **many standard deviations it is away from the mean**. This helps to **emphasize the variation** and differences in the data. In contrast, dividing by the mean would simply scale all data points proportionally without considering their spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(((faces[1] - mean_img) / std_img).ravel(), bins)\n",
    "ax.set_title('((img - mean) / std_dev) distribution')\n",
    "ax.set_xlim([-3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
